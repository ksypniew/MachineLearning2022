{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W-gV4vLYULt"
      },
      "source": [
        "# Ensemble methods. Exercises\n",
        "\n",
        "\n",
        "In this section we have only two exercise:\n",
        "\n",
        "1. Find the best three classifier in the stacking method using the classifiers from scikit-learn package.\n",
        "\n",
        "2. Build arcing arc-x4 method. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IipJ0A2TYUL2"
      },
      "outputs": [],
      "source": [
        "%store -r data_set\n",
        "%store -r labels\n",
        "%store -r test_data_set\n",
        "%store -r test_labels\n",
        "%store -r unique_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg448FiTYUL6"
      },
      "source": [
        "## Exercise 1: Find the best three classifier in the stacking method\n",
        "\n",
        "Please use the following classifiers:\n",
        "\n",
        "* Linear regression,\n",
        "* Nearest Neighbors,\n",
        "* Linear SVM,\n",
        "* Decision Tree,\n",
        "* Naive Bayes,\n",
        "* QDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNEVnyeiYUL9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0Gmz01TYUMA"
      },
      "outputs": [],
      "source": [
        "def build_classifiers():\n",
        "    \n",
        "    neighbors = KNeighborsClassifier()\n",
        "    neighbors.fit(data_set, labels)\n",
        "\n",
        "    linear_regression = LinearRegression()\n",
        "    linear_regression.fit(data_set, labels)\n",
        "\n",
        "    qda = QuadraticDiscriminantAnalysis()\n",
        "    qda.fit(data_set, labels)\n",
        "    \n",
        "    suppvec=SVC()\n",
        "    suppvec.fit(data_set,labels)\n",
        "    \n",
        "    #dt=DecisionTreeClassifier()\n",
        "    #dt.fit(data_set,labels)\n",
        "    \n",
        "    gnb=GaussianNB()\n",
        "    gnb.fit(data_set,labels)\n",
        "    \n",
        "    return neighbors, linear_regression, qda, suppvec, gnb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iw9W_aZYUME"
      },
      "outputs": [],
      "source": [
        "def build_stacked_classifier(classifiers):\n",
        "    output = []\n",
        "    for classifier in classifiers:\n",
        "        output.append(classifier.predict(data_set))\n",
        "    output = np.array(output).reshape((130,3))\n",
        "    \n",
        "    # stacked classifier part:\n",
        "    stacked_classifier = DecisionTreeClassifier()\n",
        "    stacked_classifier.fit(output.reshape((130,3)), labels.reshape((130,)))\n",
        "    test_set = []\n",
        "    for classifier in classifiers:\n",
        "        test_set.append(classifier.predict(test_data_set))\n",
        "    test_set = np.array(test_set).reshape((len(test_set[0]),3))\n",
        "    predicted = stacked_classifier.predict(test_set)\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL7Z1OrjYUMH",
        "outputId": "eaaf7233-c2d1-43bf-e419-675259281cd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.95\n",
            "[0, 2, 4]\n",
            "['neighbors', 'qda', 'gnb']\n"
          ]
        }
      ],
      "source": [
        "result=0\n",
        "wynik=[0,0,0]\n",
        "dictionary=(\"neighbors\",\"linear_reg\",\"qda\",\"svm\",\"gnb\")\n",
        "classifiers = build_classifiers()\n",
        "#print(type(classifiers))\n",
        "for i in range(5):\n",
        "    for j in range(i+1,5):\n",
        "        for k in range(j+1,5):\n",
        "            #wybrane=(i,j,k)\n",
        "            wybrane=[classifiers[m] for m in (i, j, k)]\n",
        "            #print(type(wybrane))\n",
        "            predicted = build_stacked_classifier(wybrane)\n",
        "            \n",
        "            accuracy = accuracy_score(test_labels, predicted)\n",
        "            #print(accuracy)\n",
        "            if accuracy>result:\n",
        "                result=accuracy\n",
        "                wynik=[i,j,k]\n",
        "            \n",
        "print(result)\n",
        "#print(wynik)\n",
        "print([dictionary[i] for i in wynik])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-fHt1KpYUML"
      },
      "source": [
        "## Exercise 2: \n",
        "\n",
        "Use the boosting method and change the code to fullfilt the following requirements:\n",
        "\n",
        "* the weights should be calculated as:\n",
        "$w_{n}^{(t+1)}=\\frac{1+ I(y_{n}\\neq h_{t}(x_{n})}{\\sum_{i=1}^{N}1+I(y_{n}\\neq h_{t}(x_{n})}$,\n",
        "* the prediction is done with a voting method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KqExc1HLYUMO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# prepare data set\n",
        "\n",
        "def generate_data(sample_number, feature_number, label_number):\n",
        "    data_set = np.random.random_sample((sample_number, feature_number))\n",
        "    labels = np.random.choice(label_number, sample_number)\n",
        "    return data_set, labels\n",
        "\n",
        "labels = 2\n",
        "dimension = 2\n",
        "test_set_size = 1000\n",
        "train_set_size = 5000\n",
        "train_set, train_labels = generate_data(train_set_size, dimension, labels)\n",
        "test_set, test_labels = generate_data(test_set_size, dimension, labels)\n",
        "\n",
        "# init weights\n",
        "number_of_iterations = 10\n",
        "weights = np.ones((test_set_size,)) / test_set_size\n",
        "#print(weights)\n",
        "\n",
        "def train_model(classifier, weights):\n",
        "    return classifier.fit(X=test_set, y=test_labels, sample_weight=weights)\n",
        "\n",
        "def calculate_error(model):\n",
        "    predicted = model.predict(test_set)\n",
        "    I=calculate_accuracy_vector(predicted, test_labels)\n",
        "    Z=np.sum(I)\n",
        "    return (1+Z)/1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kCZ-o7OYUMR"
      },
      "source": [
        "Fill the two functions below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XyAqzgYxYUMT"
      },
      "outputs": [],
      "source": [
        "def set_new_weights(model):\n",
        "    predicted=np.array(model.predict(test_set))\n",
        "    #print(predicted)\n",
        "    #mozemy tez skorzystac z calculate_accuracy_vector(model.predict(test_set), test_labels)\n",
        "    licznik=1+np.array(1*(predicted!=test_labels))\n",
        "    #print(licznik)\n",
        "    mianownik=np.sum(licznik)\n",
        "    weights=licznik/mianownik\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuF4tgiVYUMU"
      },
      "source": [
        "Train the classifier with the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbDrv-XPYUMV",
        "outputId": "2efd88b6-c721-40c4-ea0b-71ec61290a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00066578 0.00066578 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00133156 0.00133156\n",
            " 0.00133156 0.00133156 0.00066578 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156\n",
            " 0.00066578 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00133156 0.00133156 0.00133156 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00133156\n",
            " 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578 0.00066578\n",
            " 0.00066578 0.00066578 0.00066578 0.00133156 0.00066578 0.00133156\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00133156 0.00066578 0.00133156 0.00066578\n",
            " 0.00066578 0.00066578 0.00133156 0.00066578 0.00066578 0.00066578\n",
            " 0.00133156 0.00066578 0.00066578 0.00133156]\n"
          ]
        }
      ],
      "source": [
        "classifier = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
        "classifier.fit(X=train_set, y=train_labels)\n",
        "alphas = []\n",
        "classifiers = []\n",
        "model = train_model(classifier, weights)\n",
        "#print(set_new_weights(model))\n",
        "for iteration in range(number_of_iterations):\n",
        "    model = train_model(classifier, weights)\n",
        "    #print(weights)\n",
        "    weights = set_new_weights(model)\n",
        "    classifiers.append(model)\n",
        "\n",
        "print(weights)\n",
        "\n",
        "\n",
        "validate_x, validate_label = generate_data(1, dimension, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_3mpt9XYUMW"
      },
      "source": [
        "Set the validation data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "l60sRoytYUMW"
      },
      "outputs": [],
      "source": [
        "validate_x, validate_label = generate_data(1, dimension, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ZeNYjpYUMX"
      },
      "source": [
        "Fill the prediction code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EK_Y9yZ3YUMY"
      },
      "outputs": [],
      "source": [
        "#analogicznie jak w przypadku baggingu, ponieważ odpowiednie wagi są już uwzględnione w tablicy \"classifiers\" i przewidywania to uwzględniają\n",
        "def get_prediction(x):\n",
        "    output = []\n",
        "    for classifier in classifiers:\n",
        "        output.append(classifier.predict(x))\n",
        "    output = np.array(output)\n",
        "    predicted = []\n",
        "    for i in range(len(x)):\n",
        "        classified = output[:, i]\n",
        "        counts = np.bincount(classified)\n",
        "        predicted.append(np.argmax(counts))\n",
        "    return predicted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKRpmVWOYUMZ"
      },
      "source": [
        "Test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm06XxKnYUMa",
        "outputId": "af386019-6160-46a2-e3cc-ef51430f5b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "prediction = get_prediction(validate_x)[0]\n",
        "\n",
        "print(prediction)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "075Ensemble_Exercises.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}