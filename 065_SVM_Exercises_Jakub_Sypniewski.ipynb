{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRzhhyBZw1sY"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "In this section we have two exercises:\n",
        "1. Implement the polynomial kernel.\n",
        "2. Implement the multiclass C-SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llsBqlBMw1si"
      },
      "source": [
        "## Polynomial kernel\n",
        "\n",
        "You need to extend the ``build_kernel`` function and implement the polynomial kernel if the ``kernel_type`` is set to 'poly'. The equation that needs to be implemented:\n",
        "\\begin{equation}\n",
        "K=(X^{T}*Y)^{d}.\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkP-BGzHw1sl"
      },
      "outputs": [],
      "source": [
        "def build_kernel(data_set, kernel_type='linear'):\n",
        "    kernel = np.dot(data_set, data_set.T)\n",
        "    if kernel_type == 'rbf':\n",
        "        sigma = 1.0\n",
        "        objects_count = len(data_set)\n",
        "        b = np.ones((len(data_set), 1))\n",
        "        kernel -= 0.5 * (np.dot((np.diag(kernel)*np.ones((1, objects_count))).T, b.T)\n",
        "                         + np.dot(b, (np.diag(kernel) * np.ones((1, objects_count))).T.T))\n",
        "        kernel = np.exp(kernel / (2. * sigma ** 2))\n",
        "    elif kernel_type == 'poly':\n",
        "        #I arbitrarily set the exponent to 2 in analogy to the sigma parameter in the rbf kernel, obviously we could also add it as a parameter to the function\n",
        "        d=2\n",
        "        kernel=kernel**d\n",
        "        #if we want better precision we could use np.power(kernel,d) as it gives slightly better results than the standard operator as mentioned here https://stackoverflow.com/questions/25870923/how-to-square-or-raise-to-a-power-elementwise-a-2d-numpy-array\n",
        "    return kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSQKsyhlw1sq"
      },
      "source": [
        "## Implement a multiclass C-SVM\n",
        "\n",
        "Use the classification method that we used in notebook 7.3 and IRIS dataset to build a multiclass C-SVM classifier. Most implementation is about a function that will return the proper data set that need to be used for the prediction. You need to implement:\n",
        "- ``choose_set_for_label``\n",
        "- ``get_labels_count``"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the clarity of the code I shall implement the one vs all method\n"
      ],
      "metadata": {
        "id": "QbSUp4LLEVTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sqPNukcdw1ss"
      },
      "outputs": [],
      "source": [
        "def choose_set_for_label(data_set, label):\n",
        "    data=data_set.data\n",
        "    labels=data_set.target\n",
        "    \n",
        "    #we could do that globally and here only divide the training data into 2 classes, but I will keep the function as is not to change the return value\n",
        "    train_data_set, test_data_set, train_labels, test_labels = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=15)\n",
        "\n",
        "    #just in case label==1 or -1 so we dont get the same labels\n",
        "    bool_label=(train_labels!=label)\n",
        "    #we only change the train labels as we don't need the test_labels for our predictions\n",
        "    train_labels[bool_label]=-1\n",
        "    train_labels[~bool_label]=1\n",
        "\n",
        "\n",
        "    return train_data_set, test_data_set, train_labels, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XH3fyP4zw1sw"
      },
      "outputs": [],
      "source": [
        "def get_labels_count(data_set):\n",
        "    labels_count=len(np.unique(data_set))\n",
        "    return labels_count\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "dane=load_iris()\n",
        "print(get_labels_count(dane.target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIBtZaEdLzcg",
        "outputId": "b51549da-4c18-4641-b968-6d46774a8721"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnVpdzpew1sx"
      },
      "outputs": [],
      "source": [
        "Use the code that we have implemented earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ppKK63Snw1s0"
      },
      "outputs": [],
      "source": [
        "def train(train_data_set, train_labels, kernel_type='linear', C=10, threshold=1e-5):\n",
        "    kernel = build_kernel(train_data_set, kernel_type=kernel_type)\n",
        "\n",
        "    P = train_labels * train_labels.transpose() * kernel\n",
        "    q = -np.ones((objects_count, 1))\n",
        "    G = np.concatenate((np.eye(objects_count), -np.eye(objects_count)))\n",
        "    h = np.concatenate((C * np.ones((objects_count, 1)), np.zeros((objects_count, 1))))\n",
        "\n",
        "    A = train_labels.reshape(1, objects_count)\n",
        "    A = A.astype(float)\n",
        "    b = 0.0\n",
        "\n",
        "    sol = cvxopt.solvers.qp(cvxopt.matrix(P), cvxopt.matrix(q), cvxopt.matrix(G), cvxopt.matrix(h), cvxopt.matrix(A), cvxopt.matrix(b))\n",
        "\n",
        "    lambdas = np.array(sol['x'])\n",
        "\n",
        "    support_vectors_id = np.where(lambdas > threshold)[0]\n",
        "    vector_number = len(support_vectors_id)\n",
        "    support_vectors = train_data_set[support_vectors_id, :]\n",
        "\n",
        "    lambdas = lambdas[support_vectors_id]\n",
        "    targets = train_labels[support_vectors_id]\n",
        "\n",
        "    b = np.sum(targets)\n",
        "    for n in range(vector_number):\n",
        "        b -= np.sum(lambdas * targets * np.reshape(kernel[support_vectors_id[n], support_vectors_id], (vector_number, 1)))\n",
        "    b /= len(lambdas)\n",
        "\n",
        "    return lambdas, support_vectors, support_vectors_id, b, targets, vector_number\n",
        "\n",
        "def build_kernel(data_set, kernel_type='linear'):\n",
        "    kernel = np.dot(data_set, data_set.T)\n",
        "    if kernel_type == 'rbf':\n",
        "        sigma = 1.0\n",
        "        objects_count = len(data_set)\n",
        "        b = np.ones((len(data_set), 1))\n",
        "        kernel -= 0.5 * (np.dot((np.diag(kernel)*np.ones((1, objects_count))).T, b.T)\n",
        "                         + np.dot(b, (np.diag(kernel) * np.ones((1, objects_count))).T.T))\n",
        "        kernel = np.exp(kernel / (2. * sigma ** 2))\n",
        "    return kernel\n",
        "\n",
        "def classify_rbf(test_data_set, train_data_set, lambdas, targets, b, vector_number, support_vectors, support_vectors_id):\n",
        "    kernel = np.dot(test_data_set, support_vectors.T)\n",
        "    sigma = 1.0\n",
        "    #K = np.dot(test_data_set, support_vectors.T)\n",
        "    #kernel = build_kernel(train_data_set, kernel_type='rbf')\n",
        "    c = (1. / sigma * np.sum(test_data_set ** 2, axis=1) * np.ones((1, np.shape(test_data_set)[0]))).T\n",
        "    c = np.dot(c, np.ones((1, np.shape(kernel)[1])))\n",
        "    #aa = np.dot((np.diag(K)*np.ones((1,len(test_data_set)))).T[support_vectors_id], np.ones((1, np.shape(K)[0]))).T\n",
        "    sv = (np.diag(np.dot(train_data_set, train_data_set.T))*np.ones((1,len(train_data_set)))).T[support_vectors_id]\n",
        "    aa = np.dot(sv,np.ones((1,np.shape(kernel)[0]))).T\n",
        "    kernel = kernel - 0.5 * c - 0.5 * aa\n",
        "    kernel = np.exp(kernel / (2. * sigma ** 2))\n",
        "\n",
        "    y = np.zeros((np.shape(test_data_set)[0], 1))\n",
        "    for j in range(np.shape(test_data_set)[0]):\n",
        "        for i in range(vector_number):\n",
        "            y[j] += lambdas[i] * targets[i] * kernel[j, i]\n",
        "        y[j] += b\n",
        "    #for our multiclass classification we want the actual distance from the calculated decision boundary\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTnIQbr5w1tC",
        "outputId": "752ceca8-92d1-4509-d586-697c4fba0829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  1.1726e+02 -1.7992e+03  4e+03  2e-01  2e-15\n",
            " 1:  7.7967e+01 -1.6973e+02  3e+02  5e-03  2e-15\n",
            " 2:  1.0169e+01 -2.2323e+01  3e+01  6e-16  3e-15\n",
            " 3: -4.9607e-01 -4.9938e+00  4e+00  2e-16  1e-15\n",
            " 4: -1.4344e+00 -2.5218e+00  1e+00  2e-16  4e-16\n",
            " 5: -1.6897e+00 -2.1674e+00  5e-01  2e-16  2e-16\n",
            " 6: -1.8078e+00 -2.0073e+00  2e-01  2e-16  2e-16\n",
            " 7: -1.8464e+00 -1.9763e+00  1e-01  2e-16  2e-16\n",
            " 8: -1.8825e+00 -1.9020e+00  2e-02  2e-16  2e-16\n",
            " 9: -1.8902e+00 -1.8906e+00  4e-04  2e-16  2e-16\n",
            "10: -1.8904e+00 -1.8904e+00  5e-06  2e-16  2e-16\n",
            "11: -1.8904e+00 -1.8904e+00  5e-08  2e-16  2e-16\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  1.1410e+02 -2.4247e+03  5e+03  2e-01  2e-15\n",
            " 1:  9.0959e+01 -2.2448e+02  4e+02  6e-03  3e-15\n",
            " 2:  1.3024e+01 -2.6284e+01  4e+01  4e-05  3e-15\n",
            " 3: -1.1088e-01 -5.5940e+00  5e+00  2e-16  1e-15\n",
            " 4: -1.3519e+00 -2.4130e+00  1e+00  2e-16  5e-16\n",
            " 5: -1.5743e+00 -2.0232e+00  4e-01  2e-16  3e-16\n",
            " 6: -1.7301e+00 -1.9230e+00  2e-01  2e-16  2e-16\n",
            " 7: -1.7901e+00 -1.8551e+00  7e-02  2e-16  2e-16\n",
            " 8: -1.8107e+00 -1.8167e+00  6e-03  3e-16  2e-16\n",
            " 9: -1.8129e+00 -1.8131e+00  2e-04  4e-16  3e-16\n",
            "10: -1.8130e+00 -1.8130e+00  3e-06  2e-16  2e-16\n",
            "11: -1.8130e+00 -1.8130e+00  3e-08  2e-16  2e-16\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  1.2116e+02 -2.1371e+03  4e+03  2e-01  2e-15\n",
            " 1:  8.7753e+01 -1.8817e+02  3e+02  5e-03  2e-15\n",
            " 2:  1.1262e+01 -2.4444e+01  4e+01  2e-16  3e-15\n",
            " 3: -4.7983e-01 -5.4220e+00  5e+00  3e-16  1e-15\n",
            " 4: -1.5679e+00 -2.4236e+00  9e-01  2e-16  4e-16\n",
            " 5: -1.8591e+00 -2.2984e+00  4e-01  3e-16  2e-16\n",
            " 6: -1.9100e+00 -2.0996e+00  2e-01  2e-16  2e-16\n",
            " 7: -1.9488e+00 -2.0179e+00  7e-02  2e-16  2e-16\n",
            " 8: -1.9693e+00 -1.9745e+00  5e-03  4e-16  2e-16\n",
            " 9: -1.9712e+00 -1.9713e+00  6e-05  4e-16  2e-16\n",
            "10: -1.9713e+00 -1.9713e+00  6e-07  7e-16  2e-16\n",
            "Optimal solution found.\n",
            "0.26666666666666666\n"
          ]
        }
      ],
      "source": [
        "# modify this part \n",
        "from sklearn.model_selection import train_test_split\n",
        "import cvxopt\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "dane=load_iris()\n",
        "data_set = dane.data\n",
        "labels = dane.target\n",
        "#we divide the dataset here so we know which labels are present in the training set,using the same random_state guarantess the consistency of our calculations\n",
        "train_data_set, test_data_set, train_labels, test_labels = train_test_split(\n",
        "    data_set, labels, test_size=0.2, random_state=15)\n",
        "\n",
        "#we get the unique labels in the train_data set so we know which ones to iterate over\n",
        "labels=np.unique(train_labels)\n",
        "number_of_labels=get_labels_count(labels)\n",
        "objects_count = len(train_labels)\n",
        "\n",
        "#we initialise our classes for the first one versus all comparison\n",
        "#in general we check the distance from the hyperplane calculated in that step and if it is greater than the previously calculated maximum we classify the observation into that category as it is either positive and maximum on the right side or negative and closest to the separating hyperplane\n",
        "train_data_set, test_data_set, train_labels, test_labels=choose_set_for_label(dane,labels[0])\n",
        "lambdas, support_vectors, support_vectors_id, b, targets, vector_number = train(train_data_set, train_labels, kernel_type='rbf')\n",
        "predykcje = classify_rbf(test_data_set, train_data_set, lambdas, targets, b, vector_number, support_vectors, support_vectors_id)\n",
        "klasyfikacja=np.repeat(np.array(labels[0]),len(test_labels),axis=0)\n",
        "for i in range(number_of_labels-1):\n",
        "  train_data_set, test_data_set, train_labels, test_labels=choose_set_for_label(dane,labels[i+1])\n",
        "  lambdas, support_vectors, support_vectors_id, b, targets, vector_number = train(train_data_set, train_labels, kernel_type='rbf')\n",
        "  predicted = classify_rbf(test_data_set, train_data_set, lambdas, targets, b, vector_number, support_vectors, support_vectors_id)\n",
        "  for j in range(len(test_labels)):\n",
        "    if predicted[j]>predykcje[j]:\n",
        "      predykcje[j]=predicted[j]\n",
        "      klasyfikacja[j]=labels[i+1]\n",
        "\n",
        "\n",
        "klasyfikacja = list(klasyfikacja.astype(int))\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "#not a very good example as for the provided parameters the algorithm classifies all the iris observations into the first class\n",
        "print(accuracy_score(klasyfikacja, test_labels))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "065_SVM_Exercises.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}